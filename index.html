<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Anonymous Paper</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: 50px auto; padding: 20px; }
        h1 { text-align: center; }
        .container { background: #f9f9f9; padding: 20px; border-radius: 10px; box-shadow: 2px 2px 10px rgba(0,0,0,0.1); }
    </style>
</head>
<body>
    <div class="container">
        <h1>ICCV 2025 Submission #293</h1>
        <h2>VideoTitans: Scalable Video Prediction with Integrated Short- and Long-term Memory</h2>
        <p><strong>Abstract:</strong> We propose Video Titans, a novel memory-based architecture specifically designed for video forecasting tasks. Video Titans extends the recently introduced Titans architecture, effectively integrating a precise short-term attention mechanism with a neural long-term memory module. Traditional Transformer-based video forecasting models excel at modeling local spatial-temporal patterns, yet their quadratic computational complexity severely limits their effectiveness in handling long video sequences. To overcome these challenges, Video Titans employs a neural long-term memory module inspired by human cognitive systems, selectively encoding and adaptively forgetting historical contexts based on their informativeness. Additionally, a persistent memory module is introduced to store task-specific knowledge independently from input data, enhancing generalization and predictive stability. Experimental results demonstrate that Video Titans significantly outperforms existing Transformer and recurrent architectures in long-term video forecasting tasks, achieving superior performance while efficiently scaling to sequences of substantial length. This work establishes the effectiveness of combined short- and long-term memory mechanisms for improved scalability and accuracy in complex video understanding scenarios.</p>
    </div>
</body>
</html>
